{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\PythonProject\\chat_app\\env\\lib\\site-packages\\tensorflow\\python\\keras\\initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\PythonProject\\chat_app\\env\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "210337"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "from tensorflow import keras\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "model = keras.models.load_model('second_iter.hdf5')\n",
    "with open('tokenizer.pickle', 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)\n",
    "len(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toxicity_lvl_for_new_model(string):\n",
    "    new_text = [string]\n",
    "    new_text = tokenizer.texts_to_sequences(new_text)\n",
    "    new_text = pad_sequences(new_text, maxlen=1500)\n",
    "    prediction = model.predict(new_text)\n",
    "    print('Given String: {}'.format(string))\n",
    "    print('Toxic:         {:.0%}'.format(prediction[0][0]))\n",
    "    print('Severe Toxic:  {:.0%}'.format(prediction[0][1]))\n",
    "    print('Obscene:       {:.0%}'.format(prediction[0][2]))\n",
    "    print('Threat:        {:.0%}'.format(prediction[0][3]))\n",
    "    print('Insult:        {:.0%}'.format(prediction[0][4]))\n",
    "    print('Identity Hate: {:.0%}'.format(prediction[0][5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toxicity_lvl(string):\n",
    "    new_text = [string]\n",
    "    new_text = tokenizer.texts_to_sequences(new_text)\n",
    "    new_text = pad_sequences(new_text, maxlen=1500)\n",
    "    prediction = model.predict(new_text)\n",
    "    toxic = prediction[0][0]\n",
    "    s_toxic = prediction[0][1]\n",
    "    obs = prediction[0][2]\n",
    "    threat = prediction[0][3]\n",
    "    insult = prediction[0][4]\n",
    "    hate = prediction[0][5]\n",
    "    result = (toxic + obs + insult + hate) / 4\n",
    "    \n",
    "    print('Given String   {:}'.format(string))\n",
    "    print('Toxic:         {:}'.format(toxic))\n",
    "    print('Severe Toxic:  {:}'.format(s_toxic))\n",
    "    print('Obscene:       {:}'.format(obs))\n",
    "    print('Threat:        {:}'.format(threat))\n",
    "    print('Insult:        {:}'.format(insult))\n",
    "    print('Identity Hate: {:}'.format(hate))\n",
    "    print('Result:        {:%}'.format(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given String   You are a legend!\n",
      "Toxic:         0.13042664527893066\n",
      "Severe Toxic:  0.0012617907486855984\n",
      "Obscene:       0.008513310924172401\n",
      "Threat:        0.0003569427353795618\n",
      "Insult:        0.04870794713497162\n",
      "Identity Hate: 0.001339455135166645\n",
      "Result:        4.724684%\n"
     ]
    }
   ],
   "source": [
    "toxicity_lvl('You are a legend!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toxicity_lvl_2(string):\n",
    "    new_text = [string]\n",
    "    new_text = tokenizer.texts_to_sequences(new_text)\n",
    "    new_text = pad_sequences(new_text, maxlen=1500)\n",
    "    prediction = model.predict(new_text)\n",
    "    toxic = round((prediction[0][0] * 100), 2)\n",
    "    s_toxic = round((prediction[0][1] * 100), 2)\n",
    "    obs = round((prediction[0][2] * 100), 2)\n",
    "    threat = round((prediction[0][3] * 100), 2)\n",
    "    insult = round((prediction[0][4] * 100), 2)\n",
    "    hate = round((prediction[0][5] * 100), 2)\n",
    "    result = round((toxic + obs + insult + hate) / 4, 2)\n",
    "    print('Result: {}'.format(result))\n",
    "    if result > 50:\n",
    "        print('Offensive comment detected')\n",
    "        print('Tags related to your offensive comments: ')\n",
    "        if obs > 50:\n",
    "            print('Obscene ')\n",
    "        if insult > 50:\n",
    "            print('Insult ')\n",
    "        if hate > 50:\n",
    "            print('Hate Speech')\n",
    "        if threat > 30:\n",
    "            print('Threat')\n",
    "    else:\n",
    "        print('Good Job! No Offensive comment detected')\n",
    "    if False:\n",
    "        print('Given String   {:}'.format(string))\n",
    "        print('Toxic:         {:}%'.format(round(toxic)))\n",
    "        print('Severe Toxic:  {:}%'.format(round(s_toxic)))\n",
    "        print('Obscene:       {:}%'.format(round(obs)))\n",
    "        print('Threat:        {:}%'.format(round(threat)))\n",
    "        print('Insult:        {:}%'.format(round(insult)))\n",
    "        print('Identity Hate: {:}%'.format(round(hate)))\n",
    "        print('Result:        {:}%'.format(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: 73.78\n",
      "Offensive comment detected\n",
      "Tags related to your offensive comments: \n",
      "Obscene \n",
      "Insult \n"
     ]
    }
   ],
   "source": [
    "toxicity_lvl_2('Fuck you')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: 39.66\n",
      "Good Job! No Offensive comment detected\n"
     ]
    }
   ],
   "source": [
    "toxicity_lvl_2('You have a filthy behaviour')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: 68.26\n",
      "Offensive comment detected\n",
      "Tags related to your offensive comments: \n",
      "Obscene \n",
      "Insult \n"
     ]
    }
   ],
   "source": [
    "toxicity_lvl_2('You are an idiot!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: 19.18\n",
      "Good Job! No Offensive comment detected\n"
     ]
    }
   ],
   "source": [
    "toxicity_lvl_2('Worst behaviour')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: 6.4\n",
      "Good Job! No Offensive comment detected\n"
     ]
    }
   ],
   "source": [
    "toxicity_lvl_2('Don\\'t be so mean and rude to me')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: 1.88\n",
      "Good Job! No Offensive comment detected\n"
     ]
    }
   ],
   "source": [
    "toxicity_lvl_2('I swear i will never tell a lie')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: 81.95\n",
      "Offensive comment detected\n",
      "Tags related to your offensive comments: \n",
      "Obscene \n",
      "Insult \n",
      "Hate Speech\n"
     ]
    }
   ],
   "source": [
    "toxicity_lvl_2('gay nigga')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: 55.56\n",
      "Offensive comment detected\n",
      "Tags related to your offensive comments: \n",
      "Obscene \n",
      "Insult \n",
      "Threat\n"
     ]
    }
   ],
   "source": [
    "toxicity_lvl_2('I am going to kill you')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
